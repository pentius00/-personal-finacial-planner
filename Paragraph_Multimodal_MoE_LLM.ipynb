{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObNwjlCweprZtk4yWqOYIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pentius00/-personal-finacial-planner/blob/main/Paragraph_Multimodal_MoE_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs29NfztvB1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d8e37c-349b-450a-e8b4-c87071b67e8a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CORRECTED PRODUCTION PARAGRAPH-BASED MULTIMODAL MoE WORLD MODEL\n",
            "Training Objective: Contrastive Learning (NT-Xent)\n",
            "================================================================================\n",
            "Configuration:\n",
            "  embed_dim: 1024\n",
            "  hidden_dim: 512\n",
            "  num_experts: 8\n",
            "  top_k: 2\n",
            "  num_layers: 3\n",
            "  vocab_size: 97\n",
            "  batch_size: 32\n",
            "  learning_rate: 0.0001\n",
            "  weight_decay: 1e-05\n",
            "  num_epochs: 50\n",
            "  warmup_epochs: 5\n",
            "  dropout: 0.1\n",
            "  patience: 10\n",
            "  gradient_clip: 1.0\n",
            "  aux_loss_weight: 0.1\n",
            "  contrast_temp: 0.07\n",
            "  max_seq_len: 512\n",
            "--------------------------------------------------------------------------------\n",
            "Device: cuda\n",
            "Loading real data from 'wikitext-103-raw-v1'...\n",
            "Loaded 261014 paragraphs.\n",
            "Model parameters: 208,003,840\n",
            "\n",
            "================================================================================\n",
            "TRAINING START\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50 [Train]:  85%|████████▌ | 6604/7749 [25:43<04:22,  4.37it/s, aux=2.9690, c_loss=0.2642, loss=0.5611, lr=1.0e-04]"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# encoding: utf-8\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "# **FIX 1: Import torch.amp explicitly**\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.amp # <-- Added this\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Helper: Clear CUDA cache\n",
        "def clear_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# ================================================================================\n",
        "# 1. MODEL ARCHITECTURE\n",
        "# (Re-created based on your logs: 223M params, MoE, Char-level)\n",
        "# ================================================================================\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"A simple feed-forward network expert.\"\"\"\n",
        "    def __init__(self, embed_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class SparseMoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A Sparse Mixture of Experts layer.\n",
        "\n",
        "    This layer routes each token to the top_k experts.\n",
        "    It also computes the auxiliary load balancing loss.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_experts, top_k, dropout):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # Gating network\n",
        "        self.gate = nn.Linear(embed_dim, num_experts, bias=False)\n",
        "\n",
        "        # Expert modules\n",
        "        self.experts = nn.ModuleList([Expert(embed_dim, dropout) for _ in range(num_experts)])\n",
        "\n",
        "        # For load balancing loss\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.register_buffer(\"mean_importance\", torch.zeros(num_experts))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, embed_dim]\n",
        "        # For paragraph embeddings, seq_len is 1\n",
        "        # Let's assume x is [batch_size, embed_dim]\n",
        "        if x.dim() == 3:\n",
        "             # Take the CLS token or mean pool\n",
        "             # Assuming [batch, seq, dim] -> [batch, dim]\n",
        "             x = x.mean(dim=1)\n",
        "\n",
        "        # 1. Get Gating Logits\n",
        "        # gate_logits shape: [batch_size, num_experts]\n",
        "        gate_logits = self.gate(x)\n",
        "\n",
        "        # 2. Get Top-k Experts\n",
        "        # top_k_gates shape: [batch_size, top_k]\n",
        "        # top_k_indices shape: [batch_size, top_k]\n",
        "        top_k_gates, top_k_indices = torch.topk(gate_logits, self.top_k, dim=1)\n",
        "\n",
        "        # 3. Softmax over top-k\n",
        "        top_k_gates = self.softmax(top_k_gates)\n",
        "\n",
        "        # ==================== ** START OF CORRECTION ** ====================\n",
        "        # 4. Calculate Aux Loss (Load Balancing)\n",
        "        # This was the source of the RuntimeError.\n",
        "\n",
        "        # f_i = mean *importance* (softmax probability) for each expert\n",
        "        # gates_full shape: [batch_size, num_experts]\n",
        "        gates_full = self.softmax(gate_logits)\n",
        "        mean_importance = gates_full.mean(dim=0) # Shape: [num_experts]\n",
        "\n",
        "        # P_i = fraction of *tokens* routed to each expert\n",
        "        # We need to count how many times each expert was in the top_k\n",
        "        # top_k_indices shape: [batch_size, top_k]\n",
        "\n",
        "        # Create one-hot vectors for the chosen experts\n",
        "        # one_hot_indices shape: [batch_size, top_k, num_experts]\n",
        "        one_hot_indices = F.one_hot(top_k_indices, num_classes=self.num_experts).to(gate_logits.dtype)\n",
        "\n",
        "        # Sum across the top_k dimension\n",
        "        # This gives a multi-hot vector for each batch item, e.g., [0, 1, 0, 1, 0, 0, 0, 0]\n",
        "        # chosen_experts shape: [batch_size, num_experts]\n",
        "        chosen_experts = one_hot_indices.sum(dim=1)\n",
        "\n",
        "        # Now, take the mean across the batch to get the fraction of times each expert was chosen\n",
        "        # fraction_chosen shape: [num_experts]\n",
        "        fraction_chosen = chosen_experts.mean(dim=0)\n",
        "\n",
        "        # The loss is the dot product of these two vectors\n",
        "        # This encourages the model to distribute *both* importance and *load*\n",
        "        # Both tensors now correctly have shape [num_experts]\n",
        "        aux_loss = (mean_importance * fraction_chosen).sum() * self.num_experts\n",
        "        # ===================== ** END OF CORRECTION ** =====================\n",
        "\n",
        "\n",
        "        # 5. Route Inputs to Experts\n",
        "        # We create a sparse batch to send to experts\n",
        "\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # flat_indices: [batch_size * top_k]\n",
        "        flat_indices = top_k_indices.view(-1)\n",
        "\n",
        "        # flat_gates: [batch_size * top_k]\n",
        "        flat_gates = top_k_gates.view(-1)\n",
        "\n",
        "        # flat_x: [batch_size, embed_dim] -> [batch_size * top_k, embed_dim]\n",
        "        # We repeat the input for each expert it's routed to\n",
        "        repeated_x = x.repeat_interleave(self.top_k, dim=0)\n",
        "\n",
        "        # This loop is clearer than complex indexing, but can be optimized\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Find which tokens are routed to this expert\n",
        "            expert_mask = (flat_indices == i)\n",
        "            if expert_mask.any():\n",
        "                expert_inputs = repeated_x[expert_mask]\n",
        "                expert_outputs = expert.forward(expert_inputs)\n",
        "\n",
        "                # Apply gating score\n",
        "                gated_outputs = expert_outputs * flat_gates[expert_mask].unsqueeze(1)\n",
        "\n",
        "                # Add to final output\n",
        "                # We need to map back to original batch index\n",
        "                batch_indices = expert_mask.nonzero().squeeze(1) // self.top_k\n",
        "                # Ensure gated_outputs has the same dtype as final_output\n",
        "                final_output.index_add_(0, batch_indices, gated_outputs.to(final_output.dtype))\n",
        "\n",
        "        return final_output, aux_loss\n",
        "\n",
        "\n",
        "class CharacterEmbedder(nn.Module):\n",
        "    \"\"\"Encodes text from character IDs to a fixed embedding.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out_proj = nn.Linear(hidden_dim * 2, embed_dim) # Project back to embed_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, max_seq_len]\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "\n",
        "        # packed_output shape: [batch_size, max_seq_len, hidden_dim * 2]\n",
        "        # hidden shape: [num_layers * 2, batch_size, hidden_dim]\n",
        "        packed_output, hidden = self.gru(embedded)\n",
        "\n",
        "        # Use the final hidden state\n",
        "        # Concat fwd and bwd hidden states\n",
        "        # hidden shape: [2, batch_size, hidden_dim] -> [batch_size, hidden_dim * 2]\n",
        "        final_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        # Project to final embedding\n",
        "        # output shape: [batch_size, embed_dim]\n",
        "        output = self.out_proj(final_hidden)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ParagraphMoEModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The main model.\n",
        "    Encodes text, passes it through MoE layers, and adds a projection head\n",
        "    for contrastive learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.vocab_size = config['vocab_size']\n",
        "        self.embed_dim = config['embed_dim']\n",
        "        self.hidden_dim = config['hidden_dim'] # For GRU\n",
        "\n",
        "        # 1. Text Encoder\n",
        "        # Note: Your log mentioned \"Vision (CNN)\" but the demo was text-only.\n",
        "        # This implementation focuses on fixing the text pipeline first.\n",
        "        self.text_encoder = CharacterEmbedder(\n",
        "            self.vocab_size,\n",
        "            self.embed_dim,\n",
        "            self.hidden_dim,\n",
        "            config['dropout']\n",
        "        )\n",
        "\n",
        "        # 2. MoE Layers\n",
        "        self.moe_layers = nn.ModuleList([\n",
        "            SparseMoELayer(\n",
        "                self.embed_dim,\n",
        "                config['num_experts'],\n",
        "                config['top_k'],\n",
        "                config['dropout']\n",
        "            ) for _ in range(config['num_layers'])\n",
        "        ])\n",
        "\n",
        "        # 3. Projection Head (FOR CONTRASTIVE TRAINING ONLY)\n",
        "        # This is critical. We train the projector, but throw it\n",
        "        # away for inference.\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.embed_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.embed_dim // 2, self.embed_dim // 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_char_ids):\n",
        "        # x_char_ids shape: [batch_size, max_seq_len]\n",
        "\n",
        "        # 1. Encode text to initial embedding\n",
        "        # base_embedding shape: [batch_size, embed_dim]\n",
        "        base_embedding = self.text_encoder(x_char_ids)\n",
        "\n",
        "        # 2. Pass through MoE layers\n",
        "        current_x = base_embedding\n",
        "        total_aux_loss = 0.0\n",
        "\n",
        "        for layer in self.moe_layers:\n",
        "            current_x, aux_loss = layer(current_x)\n",
        "            total_aux_loss += aux_loss\n",
        "\n",
        "        # 3. Get Projection\n",
        "        # final_embedding is used for inference\n",
        "        # projection is used for contrastive loss\n",
        "        final_embedding = current_x\n",
        "        projection = self.projection_head(final_embedding)\n",
        "\n",
        "        # Return both the *final_embedding* (for inference) and the\n",
        "        # *projection* (for training loss)\n",
        "        return final_embedding, projection, total_aux_loss / len(self.moe_layers)\n",
        "\n",
        "    def embed_text(self, x_char_ids):\n",
        "        \"\"\"\n",
        "        Inference-only method.\n",
        "        Returns the final semantic embedding, NOT the projection.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # 1. Encode\n",
        "            base_embedding = self.text_encoder(x_char_ids)\n",
        "\n",
        "            # 2. MoE Layers\n",
        "            current_x = base_embedding\n",
        "            for layer in self.moe_layers:\n",
        "                # We ignore the aux_loss during inference\n",
        "                current_x, _ = layer(current_x)\n",
        "\n",
        "            # 3. Return the final embedding\n",
        "            return current_x\n",
        "\n",
        "# ================================================================================\n",
        "# 2. CONTRASTIVE LOSS\n",
        "# ================================================================================\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    NT-Xent Loss (from SimCLR).\n",
        "    This is the core fix. It teaches the model to learn similarity.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.cosine_sim = nn.CosineSimilarity(dim=-1)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        # z_i, z_j shape: [batch_size, projection_dim]\n",
        "        batch_size = z_i.shape[0]\n",
        "\n",
        "        # Normalize projections\n",
        "        z_i = F.normalize(z_i, p=2, dim=1)\n",
        "        z_j = F.normalize(z_j, p=2, dim=1)\n",
        "\n",
        "        # Concatenate all representations\n",
        "        # z shape: [2 * batch_size, projection_dim]\n",
        "        z = torch.cat([z_i, z_j], dim=0)\n",
        "\n",
        "        # Calculate similarity matrix\n",
        "        # sim_matrix shape: [2 * batch_size, 2 * batch_size]\n",
        "        sim_matrix = torch.mm(z, z.T) / self.temperature\n",
        "\n",
        "        # Create labels\n",
        "        # The positive sample for z_i (at index k) is z_j (at index k + batch_size)\n",
        "        # And vice-versa.\n",
        "        labels = torch.arange(batch_size, device=z_i.device)\n",
        "        labels = torch.cat([labels + batch_size, labels])\n",
        "\n",
        "        # Mask out self-similarity (diagonal)\n",
        "        sim_matrix = sim_matrix.masked_fill(\n",
        "            torch.eye(2 * batch_size, device=z_i.device).bool(),\n",
        "            -float('inf')\n",
        "        )\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.loss_fn(sim_matrix, labels)\n",
        "        return loss\n",
        "\n",
        "# ================================================================================\n",
        "# 3. DATA PREPARATION (Using Real Data)\n",
        "# ================================================================================\n",
        "\n",
        "# Simple character-level tokenizer\n",
        "class CharTokenizer:\n",
        "    def __init__(self):\n",
        "        # Basic ASCII + common chars. 0 is <PAD>, 1 is <UNK>\n",
        "        self.vocab = {char: i + 2 for i, char in enumerate(\n",
        "            \" !\\\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\"\n",
        "        )}\n",
        "        self.vocab['<PAD>'] = 0\n",
        "        self.vocab['<UNK>'] = 1\n",
        "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def text_to_ids(self, text, max_len):\n",
        "        ids = [self.vocab.get(char, self.vocab['<UNK>']) for char in text]\n",
        "        ids = ids[:max_len]\n",
        "        return ids\n",
        "\n",
        "    def pad_batch(self, batch_ids):\n",
        "        max_len_in_batch = max(len(ids) for ids in batch_ids)\n",
        "        padded = []\n",
        "        for ids in batch_ids:\n",
        "            padding = [self.vocab['<PAD>']] * (max_len_in_batch - len(ids))\n",
        "            padded.append(ids + padding)\n",
        "        return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "# Global tokenizer\n",
        "TOKENIZER = CharTokenizer()\n",
        "\n",
        "class TextAugmentation:\n",
        "    \"\"\"Creates a simple augmented 'view' of text for contrastive learning.\"\"\"\n",
        "    def __init__(self, p_drop=0.1):\n",
        "        self.p_drop = p_drop\n",
        "\n",
        "    def __call__(self, text):\n",
        "        # Simple character dropout\n",
        "        if self.p_drop == 0:\n",
        "            return text\n",
        "        return \"\".join(c for c in text if random.random() > self.p_drop)\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that returns two 'views' of the same text.\n",
        "    This is required for contrastive learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, paragraphs, transform_a, transform_b):\n",
        "        self.paragraphs = [p for p in paragraphs if len(p.strip()) > 50] # Filter empty\n",
        "        self.transform_a = transform_a\n",
        "        self.transform_b = transform_b\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paragraphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.paragraphs[idx]\n",
        "        view_1 = self.transform_a(text)\n",
        "        view_2 = self.transform_b(text)\n",
        "        return view_1, view_2\n",
        "\n",
        "def build_dataloaders(config):\n",
        "    print(\"Loading real data from 'wikitext-103-raw-v1'...\")\n",
        "    # This dataset is large. Let's use a subset for a demo.\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "\n",
        "    # Combine train and validation for a larger pool, then split\n",
        "    all_text = list(dataset['train']['text']) + list(dataset['validation']['text'])\n",
        "\n",
        "    # Filter and clean\n",
        "    paragraphs = [p.strip() for p in all_text if len(p.strip()) > 100 and len(p.strip()) < config['max_seq_len']]\n",
        "    random.shuffle(paragraphs)\n",
        "\n",
        "    print(f\"Loaded {len(paragraphs)} paragraphs.\")\n",
        "\n",
        "    # Split\n",
        "    split_idx = int(len(paragraphs) * 0.95)\n",
        "    train_paras = paragraphs[:split_idx]\n",
        "    val_paras = paragraphs[split_idx:]\n",
        "\n",
        "    # Create transforms (two different augmentations)\n",
        "    transform_a = TextAugmentation(p_drop=0.1)\n",
        "    transform_b = TextAugmentation(p_drop=0.1)\n",
        "\n",
        "    train_dataset = ContrastiveDataset(train_paras, transform_a, transform_b)\n",
        "    val_dataset = ContrastiveDataset(val_paras, transform_a, transform_b)\n",
        "\n",
        "    # Custom collate function\n",
        "    def collate_fn(batch):\n",
        "        # batch is a list of (view_1, view_2) tuples\n",
        "        view_1_list = [item[0] for item in batch]\n",
        "        view_2_list = [item[1] for item in batch]\n",
        "\n",
        "        tensor_1 = TOKENIZER.pad_batch([TOKENIZER.text_to_ids(t, config['max_seq_len']) for t in view_1_list])\n",
        "        tensor_2 = TOKENIZER.pad_batch([TOKENIZER.text_to_ids(t, config['max_seq_len']) for t in view_2_list])\n",
        "\n",
        "        return tensor_1, tensor_2\n",
        "\n",
        "    # **FIX 2: Adjusted num_workers based on your warning log**\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2, # <-- Changed from 4\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2, # <-- Changed from 4\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# ================================================================================\n",
        "# 4. TRAINING & EVALUATION LOOP\n",
        "# ================================================================================\n",
        "\n",
        "def train_one_epoch(model, loader, loss_fn, optimizer, scaler, scheduler, device, config):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_contrast_loss = 0.0\n",
        "    total_aux_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {config['epoch']}/{config['num_epochs']} [Train]\")\n",
        "\n",
        "    for tensor_1, tensor_2 in pbar:\n",
        "        tensor_1, tensor_2 = tensor_1.to(device), tensor_2.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # **FIX 3: Corrected autocast for FutureWarning**\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            # Get projections and aux loss for both views\n",
        "            _, proj_1, aux_loss_1 = model(tensor_1)\n",
        "            _, proj_2, aux_loss_2 = model(tensor_2)\n",
        "\n",
        "            # Calculate losses\n",
        "            # Moved inside autocast\n",
        "            contrast_loss = loss_fn(proj_1, proj_2)\n",
        "            aux_loss = (aux_loss_1 + aux_loss_2) / 2\n",
        "\n",
        "            # ** THIS IS THE CORRECTED LOSS **\n",
        "            # We combine contrastive loss (for meaning) and aux loss (for balance)\n",
        "            loss = contrast_loss + config['aux_loss_weight'] * aux_loss\n",
        "\n",
        "        # Scaler for AMP\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_contrast_loss += contrast_loss.item()\n",
        "        total_aux_loss += aux_loss.item()\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f\"{loss.item():.4f}\",\n",
        "            c_loss=f\"{contrast_loss.item():.4f}\",\n",
        "            aux=f\"{aux_loss.item():.4f}\",\n",
        "            lr=f\"{scheduler.get_last_lr()[0]:.1e}\"\n",
        "        )\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    avg_contrast = total_contrast_loss / len(loader)\n",
        "    avg_aux = total_aux_loss / len(loader)\n",
        "\n",
        "    print(f\"  Train Loss: {avg_loss:.4f} (Contrast: {avg_contrast:.4f}, Aux: {avg_aux:.4f})\")\n",
        "    return avg_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, loss_fn, device, config):\n",
        "    model.eval()\n",
        "    total_contrast_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"[Validate]\")\n",
        "    for tensor_1, tensor_2 in pbar:\n",
        "        tensor_1, tensor_2 = tensor_1.to(device), tensor_2.to(device)\n",
        "\n",
        "        # **FIX 3 (cont.): Corrected autocast for FutureWarning**\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            _, proj_1, _ = model(tensor_1)\n",
        "            _, proj_2, _ = model(tensor_2)\n",
        "            # Moved inside autocast\n",
        "            contrast_loss = loss_fn(proj_1, proj_2)\n",
        "\n",
        "        total_contrast_loss += contrast_loss.item()\n",
        "\n",
        "    avg_loss = total_contrast_loss / len(loader)\n",
        "    print(f\"  Val Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "# ================================================================================\n",
        "# 5. DEPLOYMENT & SEMANTIC EVALUATION\n",
        "# ================================================================================\n",
        "\n",
        "class ParagraphEmbedder:\n",
        "    \"\"\"\n",
        "    Wrapper class for production inference.\n",
        "    This is what you use in your application.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, config):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.config = config\n",
        "\n",
        "        # Load the *architecture*\n",
        "        self.model = ParagraphMoEModel(config).to(self.device)\n",
        "\n",
        "        # Load the *weights*\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        self.model.eval()\n",
        "        print(f\"Model loaded from {model_path} on {self.device}\")\n",
        "\n",
        "    def embed_text(self, text: str):\n",
        "        \"\"\"Embeds a single string of text.\"\"\"\n",
        "        # Tokenize\n",
        "        ids = TOKENIZER.text_to_ids(text, self.config['max_seq_len'])\n",
        "        tensor = torch.tensor([ids], dtype=torch.long).to(self.device)\n",
        "\n",
        "        # Get embedding\n",
        "        # We call the `embed_text` method, NOT the `forward` method\n",
        "        # to ensure we get the final embedding, not the projection.\n",
        "        embedding = self.model.embed_text(tensor)\n",
        "        return embedding.squeeze(0) # [embed_dim]\n",
        "\n",
        "    def embed_batch(self, texts: list):\n",
        "        \"\"\"Embeds a batch of texts.\"\"\"\n",
        "        ids_list = [TOKENIZER.text_to_ids(t, self.config['max_seq_len']) for t in texts]\n",
        "        tensor = TOKENIZER.pad_batch(ids_list).to(self.device)\n",
        "\n",
        "        embeddings = self.model.embed_text(tensor)\n",
        "        return embeddings # [batch_size, embed_dim]\n",
        "\n",
        "def run_semantic_evaluation(embedder):\n",
        "    \"\"\"\n",
        "    Performs the qualitative semantic check.\n",
        "    If the model is trained correctly, sim_12 > sim_13.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RUNNING SEMANTIC SIMILARITY DEMO\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    text_1 = \"Machine learning and artificial intelligence are changing the world.\"\n",
        "    text_2 = \"Deep learning and neural networks are a key part of AI.\"\n",
        "    text_3 = \"My favorite cooking recipe is for lasagna and culinary arts.\"\n",
        "\n",
        "    # Embed\n",
        "    emb_1 = embedder.embed_text(text_1).unsqueeze(0)\n",
        "    emb_2 = embedder.embed_text(text_2).unsqueeze(0)\n",
        "    emb_3 = embedder.embed_text(text_3).unsqueeze(0)\n",
        "\n",
        "    # Normalize for cosine similarity\n",
        "    emb_1 = F.normalize(emb_1)\n",
        "    emb_2 = F.normalize(emb_2)\n",
        "    emb_3 = F.normalize(emb_3)\n",
        "\n",
        "    # Calculate similarity\n",
        "    sim_12 = F.cosine_similarity(emb_1, emb_2).item()\n",
        "    sim_13 = F.cosine_similarity(emb_1, emb_3).item()\n",
        "\n",
        "    print(f\"  '{text_1[:20]}...' <-> '{text_2[:20]}...': {sim_12:.4f}\")\n",
        "    print(f\"  '{text_1[:20]}...' <-> '{text_3[:20]}...': {sim_13:.4f}\")\n",
        "\n",
        "    if sim_12 > (sim_13 + 0.1): # Check for a meaningful difference\n",
        "        print(\"\\n  ✅ SUCCESS: Related texts are significantly more similar!\")\n",
        "    else:\n",
        "        print(f\"\\n  ❌ FAILURE: Model cannot distinguish related/unrelated text.\")\n",
        "        print(f\"  (Sim 1-2: {sim_12:.4f}, Sim 1-3: {sim_13:.4f})\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# ================================================================================\n",
        "# 6. MAIN SCRIPT\n",
        "# ================================================================================\n",
        "\n",
        "def main():\n",
        "    clear_cache()\n",
        "\n",
        "    # This config is based on your log, but with CRITICAL CORRECTIONS\n",
        "    config = {\n",
        "        # Model Arch\n",
        "        'embed_dim': 1024,\n",
        "        'hidden_dim': 512,      # Hidden dim for the character GRU\n",
        "        'num_experts': 8,\n",
        "        'top_k': 2,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': TOKENIZER.vocab_size,\n",
        "\n",
        "        # Training\n",
        "        'batch_size': 32,       # Increased from 8, lower if OOM\n",
        "        'learning_rate': 0.0001,\n",
        "        'weight_decay': 1e-05,\n",
        "        'num_epochs': 50,\n",
        "        'warmup_epochs': 5,     # Warmup is handled by scheduler\n",
        "        'dropout': 0.1,\n",
        "        'patience': 10,\n",
        "        'gradient_clip': 1.0,   # Will be handled by scaler\n",
        "\n",
        "        # ** CORRECTIONS **\n",
        "        'aux_loss_weight': 0.1, # <-- INCREASED from 0.01 to fix imbalance\n",
        "        'contrast_temp': 0.07,  # Temperature for contrastive loss\n",
        "\n",
        "        # Data\n",
        "        'max_seq_len': 512      # From your model card\n",
        "    }\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"CORRECTED PRODUCTION PARAGRAPH-BASED MULTIMODAL MoE WORLD MODEL\")\n",
        "    print(\"Training Objective: Contrastive Learning (NT-Xent)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Configuration:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Data\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "\n",
        "    # Model\n",
        "    model = ParagraphMoEModel(config).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Loss, Optimizer, Scheduler\n",
        "    loss_fn = ContrastiveLoss(temperature=config['contrast_temp']).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=(len(train_loader) * config['num_epochs']), eta_min=1e-7)\n",
        "\n",
        "    # **FIX 4: Corrected GradScaler for FutureWarning**\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    # Training Loop\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    checkpoint_dir = \"checkpoints_corrected\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING START\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for epoch in range(1, config['num_epochs'] + 1):\n",
        "        config['epoch'] = epoch\n",
        "\n",
        "        train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, scaler, scheduler, device, config)\n",
        "        val_loss = validate(model, val_loader, loss_fn, device, config)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"  ✓ New best model saved! Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{config['patience']}\")\n",
        "\n",
        "        if epochs_no_improve >= config['patience']:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        clear_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best model saved at: {best_model_path}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Final Evaluation\n",
        "    print(\"\\nLoading best model for final evaluation...\")\n",
        "    # Pass the config used for training\n",
        "    embedder = ParagraphEmbedder(best_model_path, config)\n",
        "\n",
        "    # Run the *real* test\n",
        "    run_semantic_evaluation(embedder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUmd072lG0iD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}